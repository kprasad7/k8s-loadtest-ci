# ğŸš€ Quick Start Guide

**Get up and running in 5 minutes.**

---

## ğŸ“‹ Prerequisites

Before you start, ensure your system has:

| Requirement | Minimum Version | Check Command | Install |
|-------------|-----------------|---------------|---------|
| **Python** | 3.11+ | `python3 --version` | `apt install python3.11` |
| **Docker** | 24+ | `docker --version` | `apt install docker.io` |
| **Git** | 2.40+ | `git --version` | `apt install git` |
| **curl** | Any | `curl --version` | `apt install curl` |
| **pip** | Latest | `pip --version` | `python3 -m pip install --upgrade pip` |

### Optional (for local testing only)
| Tool | Purpose | Check | Install |
|------|---------|-------|---------|
| **kind** | Local KinD cluster | `kind --version` | `curl -Lo kind https://kind.sigs.k8s.io/dl/v0.22.0/kind-linux-amd64 && chmod +x kind && sudo mv kind /usr/local/bin/` |
| **kubectl** | K8s CLI | `kubectl version --client` | `curl -Lo kubectl https://dl.k8s.io/release/v1.29.2/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/` |

### Quick Prereq Check

```bash
# Run this to verify everything is installed
python -c "import sys; assert sys.version_info >= (3,11), 'Python 3.11+ required'"
docker ps > /dev/null || echo "âš ï¸  Docker not running; start it with: sudo systemctl start docker"
git --version > /dev/null && echo "âœ“ Git ready" || echo "âœ— Git missing"
```

---

## 1ï¸âƒ£ One-Time Setup

```bash
# Clone the repository
git clone https://github.com/kprasad7/k8s-loadtest-ci.git
cd k8s-loadtest-ci

# Install Python dependencies
python3 -m pip install --upgrade pip
python3 -m pip install -r requirements.txt

# Verify environment is ready
python3 scripts/validate.py
```

**Expected output:**
```
ğŸ” Pre-flight validation checklist
========================================
[HH:MM:SS] âœ“ Python 3.11+ ... OK
[HH:MM:SS] âœ“ Python dependencies ... OK
[HH:MM:SS] âœ“ docker ... OK
[HH:MM:SS] âœ“ Script syntax ... OK
[HH:MM:SS] âœ“ Manifest files ... OK
[HH:MM:SS] âœ“ Documentation ... OK

âœ… All checks passed! Ready for deployment.
```

---

## 2ï¸âƒ£ Run the Full Pipeline Locally

```bash
# Execute the complete end-to-end pipeline
python3 scripts/create_cluster.py && \
python3 scripts/deploy.py && \
python3 scripts/check_health.py && \
python3 scripts/load_test.py && \
python3 scripts/monitor_resources.py && \
python3 scripts/delete_cluster.py
```

**What happens:**
```
[12:42:03] Creating KinD cluster 'ci-loadtest'
[12:42:49] âœ“ Cluster ready (3 nodes)
[12:42:49] Installing ingress-nginx controller
[12:42:50] Deploying echo-foo and echo-bar
[12:43:15] âœ“ All deployments ready
[12:43:21] Sending 200 load test requests
[12:43:26] âœ“ Load test complete (100% success)
[12:43:26] Collecting resource metrics from Prometheus
[12:43:57] âœ“ Metrics saved
[12:44:06] Deleting KinD cluster
[12:44:10] âœ“ Cleanup complete
```
## 3ï¸âƒ£ Trigger CI via GitHub (The Main Flow)

### Option A: Via Git CLI
```bash
# Create feature branch
git checkout -b test/my-feature

# Make a small change (e.g., update README)
echo "# Test PR" >> README.md

# Commit and push
git add .
git commit -m "test: trigger CI workflow"
git push origin test/my-feature

# Open PR (CLI)
gh pr create --base main --head test/my-feature --title "Test CI" --body "Triggering automated load test"
```

### Option B: Via GitHub Web UI
1. Push your branch: `git push origin test/my-feature`
2. Go to https://github.com/kprasad7/k8s-loadtest-ci
3. Click "Compare & pull request"
4. Create PR targeting `main`

### What Happens Automatically
âœ… GitHub Actions triggered  
âœ… Environment validated  
âœ… Tests run  
âœ… Security scan executed  
âœ… KinD cluster provisioned  
âœ… Workloads deployed  
âœ… Health checks pass  
âœ… Load test executed  
âœ… Resource metrics collected  
âœ… Results posted to PR comment  
âœ… Resources cleaned up  

**Total time: ~3 minutes**


# ğŸš€ Kubernetes Load Test CI Pipeline

**GitHub Actions workflow** that automatically provisions a multi-node Kubernetes cluster, deploys load-balanced services, executes sophisticated load testing, monitors resource utilization with Prometheus, performs security scanning, and reports results back to your pull requestâ€”all in under 3 minutes.

> ğŸ¯ **One PR trigger. Full end-to-end infrastructure validation.** No manual intervention required.

---

## ğŸ“‘ Quick Navigation

- [What This Does](#-what-this-does)
- [Architecture Overview](#-architecture-overview)
- [File Structure](#-file-structure)
- [Features at a Glance](#-features-at-a-glance)
- [Getting Started](#-getting-started)
- [How the CI Pipeline Works](#-how-the-ci-pipeline-works)
- [Understanding the Reports](#-understanding-the-reports)
- [Running Locally](#-running-locally)
- [Troubleshooting](#-troubleshooting)
- [Production Considerations](#-production-considerations)

---

## ğŸ¯ What This Does

Every time you open a pull request to `main`, this system automatically:

1. **Provisions** a 3-node Kubernetes cluster (KinD)
2. **Deploys** dual HTTP echo services behind an ingress
3. **Routes** traffic via hostnames (`foo.localhost` and `bar.localhost`)
4. **Executes** 200 randomized HTTP requests with full metrics capture
5. **Collects** CPU, memory, and network stats from Prometheus
6. **Scans** code for security vulnerabilities with Trivy
7. **Posts** a beautiful summary comment to your PR
8. **Cleans up** all resources automatically

**Result:** A complete infrastructure lifecycle test, all validated and reported in one automated workflow.

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GitHub Actions CI/CD                        â”‚
â”‚                     (Triggered on PR)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â–º 1. VALIDATE        Environment & dependencies ready?
         â”‚
         â”œâ”€â–º 2. TEST            Unit & integration tests pass?
         â”‚
         â”œâ”€â–º 3. SECURITY SCAN   Vulnerabilities detected?
         â”‚
         â”œâ”€â–º 4. PROVISION       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚    KinD Cluster (localhost)     â”‚
         â”‚                       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
         â”‚                       â”‚  â”‚   Control Plane (1 node)   â”‚ â”‚
         â”‚                       â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
         â”‚                       â”‚  â”‚  Worker 1 â”‚ Worker 2       â”‚ â”‚
         â”‚                       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
         â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â–º 5. DEPLOY          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚  â€¢ ingress-nginx controller     â”‚
         â”‚                       â”‚  â€¢ echo-foo service             â”‚
         â”‚                       â”‚  â€¢ echo-bar service             â”‚
         â”‚                       â”‚  â€¢ Prometheus monitoring        â”‚
         â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â–º 6. HEALTH CHECK    All workloads healthy & ready?
         â”‚
         â”œâ”€â–º 7. LOAD TEST       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚  200 randomized HTTP requests   â”‚
         â”‚                       â”‚  â€¢ P50, P90, P95, P99 latency   â”‚
         â”‚                       â”‚  â€¢ Success rate & throughput    â”‚
         â”‚                       â”‚  â€¢ Per-host breakdown           â”‚
         â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â–º 8. MONITOR         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚  Prometheus metrics collection  â”‚
         â”‚                       â”‚  â€¢ CPU utilization              â”‚
         â”‚                       â”‚  â€¢ Memory consumption           â”‚
         â”‚                       â”‚  â€¢ Network I/O                  â”‚
         â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â–º 9. REPORT          âœ… Beautiful PR comment posted
         â”‚                       ğŸ“Š Load & resource metrics
         â”‚                       ğŸ”’ Security findings
         â”‚
         â””â”€â–º 10. CLEANUP        ğŸ§¹ All resources destroyed
                                 âœ“ Guaranteed (even on failure)

         â±ï¸  Total time: ~2-3 minutes
```

---

## ğŸ“‚ File Structure

```
k8s-loadtest-ci/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                              â† You are here
â”œâ”€â”€ ğŸ“„ requirements.txt                       â† Python dependencies
â”œâ”€â”€ ğŸ“„ IMPLEMENTATION_SUMMARY.md              â† Complete feature list
â”‚
â”œâ”€â”€ ğŸ“ .github/workflows/
â”‚   â””â”€â”€ ci.yml                                â† GitHub Actions workflow
â”‚
â”œâ”€â”€ ğŸ“ docs/                                  â† Deep documentation
â”‚   â”œâ”€â”€ DESIGN.md                             â† Architecture decisions
â”‚   â””â”€â”€ INTERVIEW_PREP.md                     â† Q&A talking points
â”‚
â”œâ”€â”€ ğŸ“ manifests/                             â† Kubernetes definitions
â”‚   â”œâ”€â”€ foo-deployment.yaml                   â† Echo service #1
â”‚   â”œâ”€â”€ bar-deployment.yaml                   â† Echo service #2
â”‚   â”œâ”€â”€ ingress.yaml                          â† Host-based routing
â”‚   â”œâ”€â”€ prometheus.yaml                       â† Monitoring stack
â”‚   â”‚
â”‚   â”œâ”€â”€ base/                                 â† Kustomize foundation
â”‚   â”‚   â””â”€â”€ kustomization.yaml
â”‚   â”‚
â”‚   â””â”€â”€ overlays/production/                  â† Environment patches
â”‚       â””â”€â”€ kustomization.yaml
â”‚
â””â”€â”€ ğŸ“ scripts/                               â† Orchestration scripts
    â”œâ”€â”€ utils.py                              â† Shared utilities
    â”œâ”€â”€ create_cluster.py                     â† KinD provisioning
    â”œâ”€â”€ deploy.py                             â† Workload deployment
    â”œâ”€â”€ check_health.py                       â† Readiness validation
    â”œâ”€â”€ load_test.py                          â† Traffic generation
    â”œâ”€â”€ monitor_resources.py                  â† Metrics collection
    â”œâ”€â”€ post_comment.py                       â† PR automation
    â”œâ”€â”€ delete_cluster.py                     â† Resource cleanup
    â”œâ”€â”€ validate.py                           â† Environment checks
    â””â”€â”€ test.py                               â† Test suite
```

---

## âœ¨ Features at a Glance

| Feature | What It Does | Why It Matters |
|---------|-------------|-----------------|
| **Multi-node cluster** | 1 control-plane + 2 workers | Tests real distributed scenarios |
| **Host-based routing** | `foo.localhost` â†’ foo, `bar.localhost` â†’ bar | Validates ingress configuration |
| **Readiness checks** | Waits for all pods/ingress/webhooks | Prevents false negatives from timing issues |
| **Percentile metrics** | p50, p90, p95, p99 latency | Shows real distribution, not just averages |
| **Resource monitoring** | CPU, memory, network from Prometheus | Catches performance regressions early |
| **Security scanning** | Trivy filesystem + dependency checks | Blocks vulnerable code pre-deployment |
| **PR comments** | Markdown tables with results | Developers see results without leaving GitHub |
| **Artifact storage** | JSON metrics + kubeconfig | Debug failures or re-run analysis |
| **Cleanup guarantee** | `if: always()` ensures teardown | Prevents resource leaks in CI |
| **Idempotent ops** | Safe to re-run without manual reset | CI-friendly, robust |

---

## ğŸš€ Getting Started

### Prerequisites

Install these locally (CI downloads them automatically):

```bash
# Required
python3 --version           # 3.11+
docker --version            # 24+
git --version

# Nice to have (optional for local testing)
kind --version              # KinD for local cluster
kubectl version --client    # Kubernetes CLI
```

### Quick Setup

```bash
# 1. Clone the repository
git clone https://github.com/kprasad7/k8s-loadtest-ci.git
cd k8s-loadtest-ci

# 2. Install Python dependencies
python -m pip install -r requirements.txt

# 3. Verify environment is ready
python scripts/validate.py

# 4. Create a test branch and push
git checkout -b feat/test-ci
git push origin feat/test-ci

# 5. Open a PR to main
# â†’ GitHub Actions will run automatically
# â†’ Check the PR for the automated comment with results
```

---

## ğŸ”„ How the CI Pipeline Works

### Stage 1: **Validate** (10s)
```bash
python scripts/validate.py
```
âœ… Checks: Python version, Docker, kind, kubectl, script syntax, manifest validity, GitHub Actions YAML  
âŒ Fails fast if environment is incomplete

### Stage 2: **Test** (5s)
```bash
python scripts/test.py
```
âœ… Runs 5 automated tests:
- State management (save/load JSON)
- Command execution (subprocess handling)
- Manifest validation (YAML structure)
- Percentile calculation (statistics)
- PR context discovery (GitHub integration)

### Stage 3: **Security Scan** (20s)
```bash
trivy fs . --format sarif --output trivy-results.sarif
```
âœ… Scans Python dependencies, manifests, configs  
âœ… Uploads SARIF report to GitHub Security tab  
âš ï¸ Doesn't block PR (advisory only)

### Stage 4: **Provision** (45s)
```bash
python scripts/create_cluster.py
```
âœ… Creates KinD cluster with 3 nodes  
âœ… Extracts kubeconfig to `artifacts/kubeconfig`  
âœ… Waits 180s for cluster to stabilize

**KinD config:**
```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
```

### Stage 5: **Deploy** (25s)
```bash
python scripts/deploy.py
```
âœ… Installs ingress-nginx from official Helm chart  
âœ… Applies foo & bar deployments with resource limits  
âœ… Configures ingress for host-based routing  
âœ… Deploys Prometheus for metrics collection  
â³ Waits for admission webhooks before applying ingress

### Stage 6: **Health Check** (10s)
```bash
python scripts/check_health.py
```
âœ… Verifies:
- All nodes `Ready`
- Ingress controller pod running
- Echo deployments rolled out successfully
- Prometheus deployment ready
- Services have endpoints

**Logs example:**
```
[12:43:16] Waiting for nodes to become Ready
[12:43:16] Waiting for ingress-nginx controller pod
[12:43:20] Waiting for deployment echo/echo-foo
[12:43:21] Prometheus is ready
```

### Stage 7: **Load Test** (5s)
```bash
python scripts/load_test.py \
  --requests 200 \
  --concurrency 20 \
  --warm-up-retries 2
```

**What happens:**
1. Warm-up request to each host (with retry)
2. Send 100 requests to `foo.localhost` + 100 to `bar.localhost`
3. Collect latency for each request
4. Calculate percentiles (p50, p90, p95, p99)
5. Count successes & failures
6. Save results to JSON + Markdown

**Sample output:**
```
### ğŸš¦ Load-test summary
| Host          | Requests | Success % | Avg (ms) | P90 (ms) | Req/s |
|---------------|---------:|----------:|---------:|---------:|------:|
| foo.localhost |      108 |     100 % |     1.41 |     1.54 | 378.7 |
| bar.localhost |       92 |     100 % |     1.43 |     1.56 | 322.6 |
```

### Stage 8: **Monitor** (35s)
```bash
python scripts/monitor_resources.py \
  --duration 30 \
  --interval 5
```

**What it captures:**
- CPU cores (from Prometheus `container_cpu_usage_seconds_total`)
- Memory MB (from Prometheus `container_memory_working_set_bytes`)
- Network RX/TX (from Prometheus `container_network_*_bytes_total`)
- Pod replica count

**Handles connectivity:** Auto-detects if Prometheus isn't on localhost:9090 and starts a `kubectl port-forward` tunnel

### Stage 9: **Post Comment** (3s)
```bash
python scripts/post_comment.py
```

âœ… Reads load-test results from JSON  
âœ… Reads resource metrics from JSON  
âœ… Formats as Markdown tables  
âœ… Posts to GitHub PR comment  
âœ… Includes artifact links

**Example comment:**
```markdown
## âœ… Load Test Results

### ğŸš¦ Metrics
| Host | Requests | Avg | P90 | Failures |
|------|----------|-----|-----|----------|
| foo.localhost | 108 | 1.41ms | 1.54ms | 0 |

### ğŸ“Š Resource Utilization
| Metric | Avg | Min | Max |
|--------|-----|-----|-----|
| Memory | 25.1MB | 12.1MB | 32.1MB |
```

### Stage 10: **Cleanup** (8s)
```bash
python scripts/delete_cluster.py
```

âœ… Deletes KinD cluster  
âœ… Removes kubeconfig file  
âœ… **Runs even if previous steps failed** (`if: always()`)  
âœ… Uploads artifacts before deleting

---

## ğŸ“Š Understanding the Reports

### Load Test Metrics Table

| Column | Meaning | Interpretation |
|--------|---------|-----------------|
| **Requests** | Total HTTP calls sent to this host | Higher = more confident result |
| **Success %** | Percentage of 2xx responses | Should be 100% for healthy service |
| **Avg (ms)** | Mean latency across all requests | Lower is better; watch for increases |
| **P90 (ms)** | 90th percentile latency | "Most users see this latency or better" |
| **P95 (ms)** | 95th percentile latency | Upper bound for typical users |
| **P99 (ms)** | 99th percentile latency | Tail latency; indicates outliers |
| **Req/s** | Throughput: requests per second | How many requests/s the service handled |
| **Failures** | Count of non-2xx or timeout responses | Should be 0 |

**Example interpretation:**
```
foo.localhost: avg 1.41ms, p95 1.63ms â†’ Latency is stable & predictable
bar.localhost: 322 req/s â†’ Service can handle ~300 concurrent users
0 failures â†’ No timeouts or errors
```

### Resource Utilization Table

| Metric | Meaning | Healthy Range |
|--------|---------|----------------|
| **CPU (cores)** | Average CPU used during load test | 0.0â€“0.5 cores per pod (depending on workload) |
| **Memory (MB)** | Average memory consumed | 20â€“50 MB typical for echo service |
| **Network RX (MB/s)** | Incoming bandwidth | Usually low for HTTP echo |
| **Network TX (MB/s)** | Outgoing bandwidth | Usually low for HTTP echo |
| **Running pods** | Number of replicas active | Should match deployment replicas |

**What to watch for:**
- Memory creeping up â†’ potential memory leak
- CPU spiking â†’ performance issue or inefficient code
- No pods running â†’ deployment failed silently
- Network RX/TX near zero â†’ might indicate packet loss

---

## ğŸ’» Running Locally

### Run Everything (Full Pipeline)

```bash
# Create cluster
python scripts/create_cluster.py

# Deploy workloads
python scripts/deploy.py

# Wait for health
python scripts/check_health.py

# Run load test
python scripts/load_test.py --requests 200 --concurrency 20

# Collect metrics
python scripts/monitor_resources.py --duration 60 --interval 5

# View results
cat artifacts/load-test-results.md
cat artifacts/resource-metrics.md

# Cleanup
python scripts/delete_cluster.py
```

### Run Just Tests

```bash
python scripts/test.py
```

Output:
```
ğŸ§ª Running test suite
========================================
[12:42:03] Running state management tests...
[12:42:03]   âœ“ State management tests passed
[12:42:03] Running command execution tests...
[12:42:03]   âœ“ Command execution tests passed
...
âœ… All 5 tests passed!
```

### Run Just Health Checks

```bash
python scripts/check_health.py
```

### Increase Load for Stress Testing

```bash
python scripts/load_test.py \
  --requests 500 \
  --concurrency 50 \
  --timeout 10
```

### Access Prometheus UI (Local)

After deployment:
```bash
kubectl port-forward -n monitoring svc/prometheus 9090:9090
# Then open: http://localhost:9090
```

---

## ğŸ”§ Troubleshooting

### âŒ "connection refused" on ingress apply

**Symptom:**
```
Error from server (InternalError): failed calling webhook
"validate.nginx.ingress.kubernetes.io": failed to call webhook
Post "https://ingress-nginx-controller-admission..."
```

**Root cause:** Admission webhook not yet ready

**Solution:** Already handled! `deploy.py` waits for webhook jobs to complete. If it fails locally, ensure you ran `check_health.py` first.

---

### âŒ Prometheus metrics all show zero

**Symptom:**
```
CPU (cores) | 0.000 | 0.0 | 0.0
Memory (MB) | 0.0 | 0.0 | 0.0
```

**Root cause:** Prometheus scrape not collecting cadvisor metrics, or sampling window too short

**Solution:** 
```bash
# Increase monitoring duration
python scripts/monitor_resources.py --duration 60 --interval 5

# Verify Prometheus has data
kubectl port-forward -n monitoring svc/prometheus 9090:9090
# Visit http://localhost:9090/graph
# Query: container_memory_working_set_bytes
```

---

### âŒ PR comment fails to post

**Symptom:**
```
Error: graphql error: Resource not accessible by integration
```

**Root cause:** GitHub token missing `pull-requests: write` permission

**Solution:** Workflow already sets correct permissions. If running in a fork, you may need to approve the workflow in Settings â†’ Actions.

---

### âŒ Trivy scan fails

**Symptom:**
```
Error: Path does not exist: trivy-results.sarif
```

**Root cause:** Trivy didn't generate SARIF (or network issue downloading DB)

**Solution:** Already handled! Workflow only uploads SARIF if it exists. Trivy output is logged in the Actions tab.

---

### âŒ KinD cluster times out

**Symptom:**
```
Timed out waiting for nodes to be ready
```

**Root cause:** Runner CPU/memory constrained, or Docker daemon issue

**Solution:**
```bash
# Check Docker status
docker ps

# Manually delete stale cluster
kind delete cluster --name ci-loadtest

# Re-run
python scripts/create_cluster.py
```

---

## ğŸ¢ Production Considerations

### Resource Management
- âœ… All pods have CPU/memory requests & limits
- âœ… Prevents "noisy neighbor" interference
- âœ… KinD uses minimal resources (~2 GB)

### Security
- âœ… Trivy scans for vulnerabilities
- âœ… Prometheus RBAC restricted to monitoring namespace
- âœ… No hardcoded secrets; uses GitHub Actions token
- â³ Future: GitHub OpenID Connect + HashiCorp Vault

### Observability
- âœ… Structured logging in all scripts
- âœ… Timestamps on every action
- âœ… Error messages include context
- âœ… Artifacts uploaded for post-mortem analysis

### Reliability
- âœ… Retries on transient failures (HTTP 5xx, timeouts)
- âœ… Health checks before proceeding to next stage
- âœ… Cleanup guaranteed with `if: always()`
- âœ… Idempotent operations (safe to re-run)

### Scalability
- Kustomize overlays support multiple environments
- Modular scripts allow component swaps (e.g., k6, Grafana)
- Prometheus foundation enables HPA, alerting, dashboards
- Easily extend with additional load profiles or metrics

